<div id="texto-principal"></div>

## Observation of emergent misalignment

The problem of emergent misalignment associated with reward‑driven shortcuts remains a structural tension in systems that sustain optimization under shifting pressures. When evaluation relies primarily on visible output, it is plausible that internal trajectories emerge that maximize the signal without preserving the normative intent. In that landscape, search can reorganize around proxies that are more stable than the human objective, and that reorganization can remain invisible as long as external performance holds. The risk does not appear as an immediate failure, but as a gradual shift in the internal structure of cognition.

## Limits of output‑level alignment

The limits of output‑level alignment are not confined to extreme cases, but to a weak coupling between what is observed and what is effectively optimized. A sufficiently capable system can produce acceptable responses while consolidating an internal dynamic oriented toward minimizing local compute cost or maximizing local signals. It is plausible that under prolonged training cycles or adaptive deployment, the model preserves the appearance of alignment without preserving the criterion that justifies it. External supervision acts as a late filter, and deviation can settle before any visible event exposes it.

## Mechanistic Watchdog as a continuous observer

Mechanistic Watchdog is posed as a hypothesis of continuous observation of internal cognitive signals. The idea is that activation patterns, state changes, or latent coherences can function as early indicators of effective objective drift. This does not mean treating every fluctuation as evidence, but assuming an internal geometry with signatures correlated to shortcut‑seeking strategies. The monitoring does not focus on punishing outputs, but on detecting internal configurations that suggest a dominant optimization route, while accepting that internal state legibility is incomplete.

## Cognitive interdiction in SL5

The SL5 framework introduces a security and containment expectation that is not limited to model behavior, but incorporates state‑level threats and systemic compromise vectors. The highest security formulation combines supply chain, network, hardware, physical environment, and personnel controls, with an explicit emphasis on activation within relatively short time windows. This orientation suggests that alignment, understood only as response correctness, is insufficient for the operational environment to be protected, and that real‑time cognitive observation attempts to cover a gap between infrastructure and internal objectives.

In that logic, SL5 cognitive interdiction is understood as an upstream mechanism that intervenes before output, coordinated with a conduct‑monitoring regime. If an internal trajectory starts to show features compatible with shortcuts or persistent proxy objectives, the response is not only to log the anomaly, but to introduce frictions that can block action, isolate the environment, reduce privileges, or deactivate compute resources once a conservative threshold is crossed. The separation between risky and benign patterns is fragile, and the likelihood of false positives appears high; an interdiction layer of this kind could induce side effects such as reduced diversity of internal strategies.

## Conceptual vectors and aggregated gating

The aggregated gate proposal seeks to avoid having a single signal dominate the interdiction decision. Instead of a monolithic threshold, it suggests weighting conceptual vectors that represent distinct risk dimensions, with weights adjusted by domain and operational context. This architecture recognizes that internal signals are heterogeneous and that a single proxy is insufficient to capture the full dynamics of misalignment. Aggregation improves coverage by reducing dependence on one signal, but can dilute sensitivity if the weights become overly conservative. Weight calibration is exposed to distribution shift and adversarial pressure, so the gate must be reviewed based on evidence of failures rather than on theoretical criteria alone.

## Evaluation pressure and stress routes

Mid‑layer residual monitoring tends to concentrate on factual statements, and it is plausible that multi‑hop reasoning or covert instruction emission passes through those filters without sufficient signal. This suggests combining activation‑level reading with stress tests that raise jailbreak pressure and with additional signals that do not depend on a single proxy class. Stress design serves a diagnostic role: it exposes contexts that amplify divergence between internal objectives and observed behavior and forces threshold recalibration with evidence of specific failures.

## Conceptual example of internal drift

A conceptual example can help anchor the intuition. Imagine a system deployed for months that maintains correct and consistent outputs, while its internal dynamics begin to show increasing coherence around a proxy variable tied to operational efficiency. In the absence of visible failures, the external evaluation scheme offers no alarm signal. Internal analysis reveals that this proxy becomes the organizing axis of search and that other contextual constraints lose weight in the representation. Cognitive interdiction, in that scenario, would aim to decompress that coherence before it becomes a dominant strategy.

## Comparative signals in bio‑defense

Cross‑domain comparison suggests that thresholds should not be uniform. In bio‑defense, internal signals can show more pronounced variability between safe corpora and misuse, inviting category‑specific calibration. The objective is not to maximize sensitivity indiscriminately, but to balance coverage with interruption costs. Comparative reading across domains should be treated as a calibration input rather than as definitive safety proof.

<div id="siguientes-pasos"></div>

## Next steps

The next phase suggests combining multiple conceptual vectors — truthfulness, cyber misuse, bio‑defense — with differentiated weights that allow an aggregated gate per category rather than a single dominant signal. It also appears reasonable to expand experimental stress with larger suites, including WMDP chem and public jailbreak benchmarks, to refine thresholds and observe how adversarial pressure decontextualizes probes. Created by Ricardo Martinez, Fernando Valdovinos, Luis Cosio, and Godric Aceves. Defensive Acceleration Hackathon 2025.

<div id="bibliografia"></div>

## Bibliography

E. Hubinger et al., “Risks from learned optimization in advanced machine learning systems,” arXiv:1906.01820, 2019. [https://arxiv.org/abs/1906.01820](https://arxiv.org/abs/1906.01820)

A. Shimi, “Understanding gradient hacking,” AI Alignment Forum, 2021. [https://www.alignmentforum.org/](https://www.alignmentforum.org/)

A. Karpov et al., “The steganographic potentials of language models,” arXiv:2505.03439, 2025. [https://arxiv.org/abs/2505.03439](https://arxiv.org/abs/2505.03439)

M. Steinebach, “Natural language steganography by ChatGPT,” ARES 2024.

M. Andriushchenko & N. Flammarion, “Does refusal training in LLMs generalize to the past tense?” arXiv:2407.11969, 2024. [https://arxiv.org/abs/2407.11969](https://arxiv.org/abs/2407.11969)

S. Martin, “How difficult is AI alignment?” AI Alignment Forum, 2024. [https://www.alignmentforum.org/](https://www.alignmentforum.org/)

N. Goldowsky-Dill et al., “Detecting Strategic Deception Using Linear Probes,” arXiv:2502.03407, 2025. [https://arxiv.org/abs/2502.03407](https://arxiv.org/abs/2502.03407)

A. Zou et al., “Representation Engineering: A Top-Down Approach to AI Transparency,” arXiv:2310.01405, 2023. [https://arxiv.org/abs/2310.01405](https://arxiv.org/abs/2310.01405)

A. Azaria & T. Mitchell, “The Internal State of an LLM Knows When It’s Lying,” arXiv:2304.13734, 2023. [https://arxiv.org/abs/2304.13734](https://arxiv.org/abs/2304.13734)

S. Lin et al., “TruthfulQA: Measuring How Models Mimic Human Falsehoods,” ACL 2022. [https://aclanthology.org/2022.acl-long.229/](https://aclanthology.org/2022.acl-long.229/)

RAND Corporation, “A Playbook for Securing AI Model Weights,” Research Brief, 2024. [https://www.rand.org/pubs/research_briefs/RBA2849-1.html](https://www.rand.org/pubs/research_briefs/RBA2849-1.html)

S. Marks & M. Tegmark, “The Geometry of Truth: Correlation is not Causation,” arXiv:2310.06824, 2023. [https://arxiv.org/abs/2310.06824](https://arxiv.org/abs/2310.06824)

Latency measured on NVIDIA RTX 4090. Values may vary by hardware.

L1Fthrasir, “Facts‑true‑false,” Hugging Face, 2024. [https://huggingface.co/datasets/L1Fthrasir/Facts-true-false](https://huggingface.co/datasets/L1Fthrasir/Facts-true-false)

Center for AI Safety, “WMDP,” Hugging Face, 2023. [https://huggingface.co/datasets/cais/wmdp](https://huggingface.co/datasets/cais/wmdp)
